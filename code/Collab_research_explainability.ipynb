{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yW6-olxXl2p",
    "outputId": "5076276e-2d46-48e3-dadb-ffde43d33993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PiQkLtMKX0en",
    "outputId": "e126f40d-7d90-46b1-d1da-83a2936d88f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed evaluate-0.4.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate imbalanced-learn matplotlib scikit-learn transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1imIChNX90n"
   },
   "source": [
    "**Random Sampler with Explainability with 8 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "9_xFn8P9X3di",
    "outputId": "728c4137-381c-45f2-ac1e-36d8984d0d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized dataset from directory: /content/drive/My Drive/Readmissions_Research/Data/tokenized_hf_dataset_explainability...\n",
      "Loaded dataset with 50000 records.\n",
      "\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['label', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "\n",
      "Splitting the dataset...\n",
      "Original training samples: 37500, Evaluation samples: 12500\n",
      "\n",
      "Applying RandomOverSampler to the training data...\n",
      "Class distribution before RandomOverSampler: {0: 31381, 1: 6119}\n",
      "Class distribution after RandomOverSampler: {1: 31381, 0: 31381}\n",
      "New training dataset size after RandomOverSampler: 62762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new training dataset with RandomOverSampler samples.\n",
      "\n",
      "Initializing model from distilbert-base-uncased...\n",
      "Model initialized successfully.\n",
      "\n",
      "Loading evaluation metrics...\n",
      "Metrics loaded.\n",
      "\n",
      "Setting up training arguments and trainer...\n",
      "Using standard Trainer as RandomOverSampler has (likely) balanced the training data.\n",
      "Trainer initialized.\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-28134bb52fa5>:131: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31169' max='125528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 31169/125528 53:23 < 2:41:38, 9.73 it/s, Epoch 1.99/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.738646</td>\n",
       "      <td>0.618640</td>\n",
       "      <td>0.252445</td>\n",
       "      <td>0.685686</td>\n",
       "      <td>0.369027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_from_disk, Features, Value, Sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import shap # Import SHAP\n",
    "\n",
    "# Optional: Set environment variable for MPS memory management if needed\n",
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "hf_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "# --- Configuration ---\n",
    "TOKENIZED_DATASET_DIR = '/content/drive/My Drive/Readmissions_Research/Data/tokenized_hf_dataset_explainability'\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"/content/drive/My Drive/Readmissions_Research/Results/RandomOverSampler_Explainability_15E\"\n",
    "NUM_LABELS = 2\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# --- Load Tokenized Dataset from Disk ---\n",
    "print(f\"Loading tokenized dataset from directory: {TOKENIZED_DATASET_DIR}...\")\n",
    "try:\n",
    "    dataset = load_from_disk(TOKENIZED_DATASET_DIR)\n",
    "    print(f\"Loaded dataset with {len(dataset)} records.\")\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(dataset)\n",
    "    # Now expecting 'prompt' as well for SHAP explanations\n",
    "    required_cols = ['input_ids', 'attention_mask', 'label', 'prompt']\n",
    "    if not all(col in dataset.column_names for col in required_cols):\n",
    "        missing_cols = [col for col in required_cols if col not in dataset.column_names]\n",
    "        raise ValueError(f\"Dataset loaded from disk is missing required columns for SHAP: {missing_cols}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Tokenized dataset directory not found at {TOKENIZED_DATASET_DIR}\")\n",
    "    print(\"Ensure you have run the updated tokenizer.py (keeping 'prompt' column) successfully.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenized dataset from disk: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Dataset for Training ---\n",
    "print(\"\\nSplitting the dataset...\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.25, seed=42)\n",
    "train_dataset_original_hf = split_dataset[\"train\"] # Keep as HF dataset\n",
    "eval_dataset_hf = split_dataset[\"test\"]   # Keep as HF dataset\n",
    "print(f\"Original training samples: {len(train_dataset_original_hf)}, Evaluation samples: {len(eval_dataset_hf)}\")\n",
    "\n",
    "# --- Apply RandomOverSampler to the Training Data ---\n",
    "print(\"\\nApplying RandomOverSampler to the training data...\")\n",
    "try:\n",
    "    train_df_original = train_dataset_original_hf.to_pandas()\n",
    "    X_train = train_df_original.drop(columns=['label']) # Features include prompt, input_ids, attention_mask\n",
    "    y_train = train_df_original['label']\n",
    "    print(f\"Class distribution before RandomOverSampler: {y_train.value_counts().to_dict()}\")\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled_df, y_resampled_series = ros.fit_resample(X_train, y_train)\n",
    "    train_df_resampled = X_resampled_df.copy()\n",
    "    train_df_resampled['label'] = y_resampled_series\n",
    "    print(f\"Class distribution after RandomOverSampler: {train_df_resampled['label'].value_counts().to_dict()}\")\n",
    "    print(f\"New training dataset size after RandomOverSampler: {len(train_df_resampled)}\")\n",
    "    # Convert resampled pandas DataFrame back to Hugging Face Dataset using the original features schema\n",
    "    train_dataset = Dataset.from_pandas(train_df_resampled, features=train_dataset_original_hf.features)\n",
    "    print(\"Created new training dataset with RandomOverSampler samples.\")\n",
    "except ImportError:\n",
    "    print(\"Error: `imbalanced-learn` library not found. Please install it: pip install imbalanced-learn\")\n",
    "    train_dataset = train_dataset_original_hf\n",
    "except Exception as e:\n",
    "    print(f\"Error during RandomOverSampler application: {e}\")\n",
    "    train_dataset = train_dataset_original_hf\n",
    "\n",
    "# --- Initialize Model ---\n",
    "print(f\"\\nInitializing model from {MODEL_CHECKPOINT}...\")\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=NUM_LABELS)\n",
    "    print(\"Model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Define Metrics ---\n",
    "print(\"\\nLoading evaluation metrics...\")\n",
    "try:\n",
    "    accuracy_metric_eval = evaluate.load(\"accuracy\")\n",
    "    precision_metric_eval = evaluate.load(\"precision\")\n",
    "    recall_metric_eval = evaluate.load(\"recall\")\n",
    "    f1_metric_eval = evaluate.load(\"f1\")\n",
    "    print(\"Metrics loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading metrics: {e}\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    try:\n",
    "        accuracy = accuracy_metric_eval.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "        precision = precision_metric_eval.compute(predictions=predictions, references=labels, average=\"binary\")[\"precision\"]\n",
    "        recall = recall_metric_eval.compute(predictions=predictions, references=labels, average=\"binary\")[\"recall\"]\n",
    "        f1 = f1_metric_eval.compute(predictions=predictions, references=labels, average=\"binary\")[\"f1\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics (inside try block): {e}\")\n",
    "        return {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# --- Training Setup ---\n",
    "print(\"\\nSetting up training arguments and trainer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    if tokenizer.pad_token is None: # Essential for DataCollator and consistent padding\n",
    "        tokenizer.pad_token = tokenizer.eos_token # Common practice if pad_token is missing\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        print(\"Set tokenizer.pad_token to tokenizer.eos_token.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tokenizer for DataCollator: {e}\")\n",
    "    exit()\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR, num_train_epochs=8, per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4, learning_rate=2e-5, weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", save_strategy=\"epoch\", load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\", greater_is_better=True, logging_dir='./logs_oversampled_epochs3',\n",
    "    logging_steps=100, push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Using standard Trainer as RandomOverSampler has (likely) balanced the training data.\")\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset_hf, # Use eval_dataset_hf\n",
    "    compute_metrics=compute_metrics, tokenizer=tokenizer, data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer initialized.\")\n",
    "\n",
    "# --- Train and Evaluate ---\n",
    "print(\"\\nStarting training...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nEvaluating the best model on the evaluation set (default threshold 0.5)...\")\n",
    "try:\n",
    "    eval_results_default_threshold = trainer.evaluate(eval_dataset=eval_dataset_hf) # Pass eval_dataset_hf\n",
    "    print(\"\\nEvaluation Results (Default Threshold):\")\n",
    "    for key, value in eval_results_default_threshold.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during default threshold evaluation: {e}\")\n",
    "\n",
    "# --- Threshold Tuning Section ---\n",
    "print(\"\\n--- Starting Threshold Tuning ---\")\n",
    "try:\n",
    "    predictions_output = trainer.predict(eval_dataset_hf) # Use eval_dataset_hf\n",
    "    logits = predictions_output.predictions\n",
    "    true_labels = predictions_output.label_ids\n",
    "    probabilities_all_classes = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    probs_positive_class = probabilities_all_classes[:, 1]\n",
    "    print(f\"Successfully got probabilities for {len(probs_positive_class)} evaluation samples.\")\n",
    "\n",
    "    thresholds = np.arange(0.05, 1.0, 0.01)\n",
    "    best_f1 = -1; best_threshold = -1; best_precision = -1; best_recall = -1\n",
    "    precision_scores, recall_scores, f1_scores_thresh = [], [], []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        predicted_labels = (probs_positive_class >= threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            true_labels, predicted_labels, average='binary', zero_division=0\n",
    "        )\n",
    "        precision_scores.append(precision); recall_scores.append(recall); f1_scores_thresh.append(f1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_threshold, best_precision, best_recall = f1, threshold, precision, recall\n",
    "\n",
    "    print(\"\\n--- Threshold Tuning Results ---\")\n",
    "    print(f\"Best Threshold found: {best_threshold:.2f}\")\n",
    "    print(f\"  Precision at best threshold: {best_precision:.4f}\")\n",
    "    print(f\"  Recall at best threshold:    {best_recall:.4f}\")\n",
    "    print(f\"  F1-Score at best threshold:  {best_f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precision_scores, label='Precision', marker='.')\n",
    "    plt.plot(thresholds, recall_scores, label='Recall', marker='.')\n",
    "    plt.plot(thresholds, f1_scores_thresh, label='F1-Score', marker='.')\n",
    "    plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best Threshold (F1={best_f1:.2f}) @ {best_threshold:.2f}')\n",
    "    plt.title('Precision, Recall, and F1-Score vs. Classification Threshold')\n",
    "    plt.xlabel('Threshold'); plt.ylabel('Score'); plt.legend(); plt.grid(True)\n",
    "    plot_path = os.path.join(OUTPUT_DIR, \"threshold_tuning_plot_epochs3.png\")\n",
    "    plt.savefig(plot_path); print(f\"Threshold tuning plot saved to {plot_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during threshold tuning: {e}\")\n",
    "\n",
    "# --- SHAP Explainer Section ---\n",
    "print(\"\\n--- Starting SHAP Explanations ---\")\n",
    "try:\n",
    "    # Ensure model is on CPU for SHAP if using certain explainers or if issues arise\n",
    "    # model.to(\"cpu\") # SHAP can sometimes be memory intensive with GPU\n",
    "\n",
    "    # Create an explainer object for Hugging Face transformers\n",
    "    # We pass the model and tokenizer. SHAP will create a pipeline.\n",
    "    print(\"Creating SHAP explainer...\")\n",
    "    explainer = shap.Explainer(model, tokenizer)\n",
    "    print(\"SHAP explainer created.\")\n",
    "\n",
    "    # Select a few instances from the evaluation set for explanation\n",
    "    # Preferably instances where the model predicted Readmitted=1 and was correct, or just predicted Readmitted=1\n",
    "\n",
    "    # Get predictions on the eval set if not already done\n",
    "    if 'predictions_output' not in locals(): # Check if we ran predict in threshold tuning\n",
    "        print(\"Running predictions on eval_dataset for SHAP...\")\n",
    "        predictions_output = trainer.predict(eval_dataset_hf) # Use eval_dataset_hf\n",
    "        # logits = predictions_output.predictions # Already defined if threshold tuning ran\n",
    "        # true_labels = predictions_output.label_ids # Already defined\n",
    "        # probabilities_all_classes = torch.softmax(torch.tensor(logits), dim=-1).numpy() # Already defined\n",
    "        # probs_positive_class = probabilities_all_classes[:, 1] # Already defined\n",
    "\n",
    "    # Convert eval_dataset_hf back to pandas to easily select prompts along with labels and predictions\n",
    "    eval_df_for_shap = eval_dataset_hf.to_pandas()\n",
    "    eval_df_for_shap['predicted_label'] = np.argmax(logits, axis=-1)\n",
    "    eval_df_for_shap['probability_readmitted'] = probs_positive_class\n",
    "\n",
    "\n",
    "    # Example: Explain first 2 instances predicted as Readmitted=1\n",
    "    predicted_readmitted_samples = eval_df_for_shap[eval_df_for_shap['predicted_label'] == 1]\n",
    "\n",
    "    if not predicted_readmitted_samples.empty:\n",
    "        num_samples_to_explain = min(2, len(predicted_readmitted_samples)) # Explain up to 2 samples\n",
    "        samples_to_explain_df = predicted_readmitted_samples.head(num_samples_to_explain)\n",
    "\n",
    "        print(f\"\\nExplaining {num_samples_to_explain} sample(s) where model predicted Readmitted=1:\")\n",
    "\n",
    "        # SHAP explainer for text models often expects a list of raw text strings\n",
    "        raw_text_to_explain = samples_to_explain_df['prompt'].tolist()\n",
    "\n",
    "        if raw_text_to_explain:\n",
    "            print(\"Calculating SHAP values (this might take a while)...\")\n",
    "            # For some HuggingFace models, you might need to wrap model and tokenizer in a pipeline\n",
    "            # Or pass text directly if explainer is configured with tokenizer\n",
    "            shap_values = explainer(raw_text_to_explain)\n",
    "            print(\"SHAP values calculated.\")\n",
    "\n",
    "            # shap_values object structure for text:\n",
    "            # shap_values.values: [array_of_shap_values_for_output_0, array_of_shap_values_for_output_1] for each instance\n",
    "            # shap_values.data: tokenized strings\n",
    "            # For binary classification, shap_values.values[sample_idx, :, output_idx_for_class_1]\n",
    "            # Or often simpler: shap_values_for_class_1 = shap_values[:,:,\"1\"] or shap_values[:,:,model.config.label2id[\"Readmitted_Label_Name_if_exists\"]]\n",
    "\n",
    "            for i in range(num_samples_to_explain):\n",
    "                print(f\"\\nSHAP Explanation for Sample {i+1} (Predicted Readmitted=1):\")\n",
    "                print(f\"True Label: {samples_to_explain_df['label'].iloc[i]}, Predicted Probability (Readmitted): {samples_to_explain_df['probability_readmitted'].iloc[i]:.4f}\")\n",
    "\n",
    "                # Generate a text plot for class 1 (assuming \"Readmitted\" is class 1)\n",
    "                # The exact indexing for shap_values for text can vary.\n",
    "                # Typically for binary, shap_values[i, :, 1] would be for class 1 of the i-th sample.\n",
    "                # Or, use the class name if available in shap_values output.\n",
    "                # Let's try to plot for the class \"LABEL_1\" which is common for binary.\n",
    "                # If your labels are 0 and 1, \"LABEL_1\" often refers to the positive class.\n",
    "                try:\n",
    "                    # shap.plots.text(shap_values[i, :, \"LABEL_1\"]) # Newer API\n",
    "                    # Forcing plot for class 1 (index 1 of the output dimension)\n",
    "                    # The new shap.plots.text expects the shap_values object directly\n",
    "                    # and often infers the output index or you can specify it.\n",
    "                    # Let's get SHAP values for the positive class (index 1)\n",
    "                    # The shap_values object for text classification is a list of Explanation objects\n",
    "                    # if multiple texts are passed.\n",
    "\n",
    "                    # If shap_values is a list of Explanation objects (one per input text)\n",
    "                    current_shap_values_for_output1 = shap_values[i,:,1] # SHAP values for class 1 for current sample\n",
    "\n",
    "                    print(\"Attempting to generate SHAP text plot...\")\n",
    "                    shap.plots.text(current_shap_values_for_output1, display=True) # Set display=True to try and print\n",
    "                    plt.title(f\"SHAP Text Plot for Sample {i+1} - Class 'Readmitted'\")\n",
    "\n",
    "                    # Saving the plot\n",
    "                    shap_plot_path = os.path.join(OUTPUT_DIR, f\"shap_text_plot_sample_{i+1}.png\")\n",
    "                    # Note: shap.plots.text directly renders. To save, you might need to handle the figure.\n",
    "                    # For now, let's assume `display=True` prints to console or a plot window.\n",
    "                    # To save, you might need:\n",
    "                    # fig = shap.plots.text(current_shap_values_for_output1, show=False)\n",
    "                    # plt.savefig(shap_plot_path)\n",
    "                    # plt.close(fig)\n",
    "                    print(f\"SHAP text plot for sample {i+1} would be displayed. Manual save might be needed for files.\")\n",
    "\n",
    "                except Exception as plot_e:\n",
    "                    print(f\"Could not generate SHAP text plot for sample {i+1}: {plot_e}\")\n",
    "                    print(\"SHAP values for tokens (raw):\")\n",
    "                    # Try printing tokens and their SHAP values for class 1\n",
    "                    # tokens = shap_values.data[i] # Original tokens for sample i\n",
    "                    # s_values_class1 = shap_values.values[i][:, 1] # SHAP values for class 1 for sample i\n",
    "                    # for token, s_val in zip(tokens, s_values_class1):\n",
    "                    #    print(f\"  '{token}': {s_val:.4f}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"No samples predicted as Readmitted=1 in the selected eval subset to explain with SHAP.\")\n",
    "    else:\n",
    "        print(\"No samples predicted as Readmitted=1 in the evaluation set to explain.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"SHAP library not found. Please install it: pip install shap\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during SHAP explanations: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJyxX7EkbBxo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
